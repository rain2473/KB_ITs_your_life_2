{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e281196",
   "metadata": {},
   "source": [
    "# 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d9999",
   "metadata": {},
   "source": [
    "## 보팅 voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290f92c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard Train Accuracy : 98.12%\n",
      "hard Test Accuracy : 95.10%\n",
      "\n",
      "soft Train Accuracy : 99.53%\n",
      "soft Test Accuracy : 95.80%\n",
      "\n",
      "knn1 Train Accuracy : 94.60%\n",
      "knn1 Test Accuracy : 91.61%\n",
      "\n",
      "knn2 Train Accuracy : 95.77%\n",
      "knn2 Test Accuracy : 91.61%\n",
      "\n",
      "lr Train Accuracy : 96.71%\n",
      "lr Test Accuracy : 93.71%\n",
      "\n",
      "dt3 Train Accuracy : 97.65%\n",
      "dt3 Test Accuracy : 93.01%\n",
      "\n",
      "dt5 Train Accuracy : 100.00%\n",
      "dt5 Test Accuracy : 92.31%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# voting 실습\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 데이터 불러오기\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "y = cancer['target']\n",
    "x = cancer['data']\n",
    "\n",
    "# 데이터 분할\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    stratify=cancer['target'],\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# voting 참여 분류기 생성\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=3)\n",
    "lr = LogisticRegression(max_iter=10000)                           \n",
    "dt3 = DecisionTreeClassifier(max_depth=3)\n",
    "dt5 = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# voting 모델 생성\n",
    "hard = VotingClassifier([('knn1',knn1),('knn2',knn2),('lr',lr),('dt3',dt3),('dt5',dt5)])\n",
    "soft = VotingClassifier([('knn1',knn1),('knn2',knn2),('lr',lr),('dt3',dt3),('dt5',dt5)], voting='soft')\n",
    "\n",
    "# voting 분류기 성능 평가\n",
    "names = ['hard','soft','knn1','knn2','lr','dt3','dt5']\n",
    "\n",
    "for idx, model in enumerate([hard,soft,knn1,knn2,lr,dt3,dt5]):\n",
    "    model.fit(x_tr,y_tr)\n",
    "    name = names[idx]\n",
    "    tr_score = model.score(x_tr,y_tr) * 100\n",
    "    te_score = model.score(x_te,y_te) * 100\n",
    "    print(f'{name} Train Accuracy : {tr_score:.2f}%')\n",
    "    print(f'{name} Test Accuracy : {te_score:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63132545",
   "metadata": {},
   "source": [
    "## 배깅 bagging\n",
    "대표적 : Random Forest  \n",
    "장점 : 과적합 줄임. 스케일 조절 필요 없음, 결정 트리 모델 예측 성능 유지, 매개변수의 튜닝을 많이 하지 않아도 됨.  \n",
    "단점 : 결정트리 많이 필요, 대량 데이터 셋이라면 다소 시간 걸림, 텍스트 데이터 등 희소&고차원 데이터(one-hot)엔 작동 잘 안됨.  \n",
    "랜덤으로 결정트리를 많이 많들어 가장 많이 나온 답을 선택 => 랜덤+포레스트(트리 * 많이) 결정트리 집단지성ㅋㅋ  \n",
    "n_estimators : 결정트리의 갯수  \n",
    "max_features : 선택할 무작위 특성의 개수, 핵심변수, 기본값을 권장.  \n",
    "hyperParametor : 모델링할 때 사용자가 직접 세팅해주는 값==> 성능에 영향  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff5d6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.958041958041958)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=5).fit(x_tr,y_tr)\n",
    "model.score(x_tr,y_tr), model.score(x_te,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "171623df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9976525821596244, 0.951048951048951)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=4).fit(x_tr,y_tr)\n",
    "model.score(x_tr,y_tr), model.score(x_te,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c61f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9835680751173709, 0.9300699300699301)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=3).fit(x_tr,y_tr)\n",
    "model.score(x_tr,y_tr), model.score(x_te,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8006a899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9624413145539906, 0.9230769230769231)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=2).fit(x_tr,y_tr)\n",
    "model.score(x_tr,y_tr), model.score(x_te,y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccfe72",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "여러개의 결정트리를 묶어 강력한 모델을 만드는 방법  \n",
    "뎁스가 얕은 트리를 많이 연결(1~5 뎁)  \n",
    "이전 트리 오차 보완하는 형식  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fcda9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.951048951048951)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier().fit(x_tr,y_tr)\n",
    "model.score(x_tr,y_tr), model.score(x_te,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e64efdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [('rf',RandomForestClassifier()),('gb',GradientBoostingClassifier())]\n",
    "\n",
    "model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "model.fit(x_tr,y_tr).score(x_te,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f05518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard Train Accuracy : 100.00%\n",
      "hard Test Accuracy : 97.59%\n",
      "\n",
      "soft Train Accuracy : 100.00%\n",
      "soft Test Accuracy : 98.15%\n",
      "\n",
      "knn1 Train Accuracy : 99.20%\n",
      "knn1 Test Accuracy : 97.96%\n",
      "\n",
      "knn2 Train Accuracy : 100.00%\n",
      "knn2 Test Accuracy : 96.67%\n",
      "\n",
      "lr Train Accuracy : 47.18%\n",
      "lr Test Accuracy : 47.59%\n",
      "\n",
      "dt3 Train Accuracy : 100.00%\n",
      "dt3 Test Accuracy : 96.30%\n",
      "\n",
      "dt5 Train Accuracy : 97.22%\n",
      "dt5 Test Accuracy : 94.81%\n",
      "\n",
      "gb Train Accuracy : 100.00%\n",
      "gb Test Accuracy : 97.04%\n",
      "\n",
      "best model is soft\n",
      "Test Accuracy : 98.15%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# 데이터 세트 준비\n",
    "\n",
    "digits = load_digits()\n",
    "y = digits['target']\n",
    "x = digits['data']\n",
    "\n",
    "# 데이터 분할\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    stratify=digits['target'],\n",
    "    random_state=0,\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "# 모델 평가(가장 좋은 Classfication 모델 찾기)\n",
    "\n",
    "# voting 참여 분류기 생성\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "lr = LogisticRegression(max_iter=10000)                           \n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "rf = RandomForestClassifier(max_depth=5)\n",
    "gb = GradientBoostingClassifier()\n",
    "estimators = [('rf',RandomForestClassifier()),('gb',GradientBoostingClassifier())]\n",
    "sc = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "\n",
    "# voting 모델 생성\n",
    "hard = VotingClassifier([('knn',knn),('lr',lr),('dt',dt),('gb',gb),('rf',rf),('sc',sc)])\n",
    "soft = VotingClassifier([('knn',knn),('lr',lr),('dt',dt),('gb',gb),('rf',rf),('sc',sc)], voting='soft')\n",
    "\n",
    "# voting 분류기 성능 평가\n",
    "names = ['hard','soft','knn1','knn2','lr','dt3','dt5','gb','rf','sc']\n",
    "\n",
    "max_score=0\n",
    "best_model = ''\n",
    "\n",
    "for idx, model in enumerate([hard,soft,knn,lr,dt,gb,rf,sc]):\n",
    "    \n",
    "    model.fit(x_tr,y_tr)\n",
    "    name = names[idx]\n",
    "    tr_score = model.score(x_tr,y_tr) * 100\n",
    "    te_score = model.score(x_te,y_te) * 100\n",
    "    print(f'{name} Train Accuracy : {tr_score:.2f}%')\n",
    "    print(f'{name} Test Accuracy : {te_score:.2f}%')\n",
    "    \n",
    "    if te_score > max_score:\n",
    "        max_score = te_score\n",
    "        best_model = str(name)\n",
    "    print()\n",
    "    \n",
    "print(f'best model is {best_model}')\n",
    "print(f'Test Accuracy : {max_score:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b264f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>98.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>97.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn1</th>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn2</th>\n",
       "      <td>98.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>96.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt3</th>\n",
       "      <td>46.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt5</th>\n",
       "      <td>67.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging, max_depth=1</th>\n",
       "      <td>0.695556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging, max_depth=2</th>\n",
       "      <td>0.826667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging, max_depth=3</th>\n",
       "      <td>0.868889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging, max_depth=4</th>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging, max_depth=5</th>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosting</th>\n",
       "      <td>0.968889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacking</th>\n",
       "      <td>0.973333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0\n",
       "hard                  98.222222\n",
       "soft                  97.777778\n",
       "knn1                  98.000000\n",
       "knn2                  98.666667\n",
       "lr                    96.444444\n",
       "dt3                   46.888889\n",
       "dt5                   67.555556\n",
       "bagging, max_depth=1   0.695556\n",
       "bagging, max_depth=2   0.826667\n",
       "bagging, max_depth=3   0.868889\n",
       "bagging, max_depth=4   0.920000\n",
       "bagging, max_depth=5   0.940000\n",
       "boosting               0.968889\n",
       "stacking               0.973333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "best_model = {}\n",
    "\n",
    "# 데이터 분할\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(digits['data'],digits['target'],stratify=digits['target'],random_state=0)\n",
    "\n",
    "# 모델 설정\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=3)\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "dt3 = DecisionTreeClassifier(max_depth=3)\n",
    "dt5 = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "hard = VotingClassifier([('knn1', knn1), ('knn2', knn2), ('lr', lr), ('dt3', dt3), ('dt5', dt5)])\n",
    "\n",
    "soft = VotingClassifier([('knn1', knn1), ('knn2', knn2), ('lr', lr), ('dt3', dt3), ('dt5', dt5)], voting='soft')\n",
    "\n",
    "names = ['hard', 'soft', 'knn1', 'knn2', 'lr', 'dt3', 'dt5']\n",
    "for idx, model in enumerate([hard, soft, knn1, knn2, lr, dt3, dt5]):\n",
    "    model.fit(x_tr, y_tr)\n",
    "    name = names[idx]\n",
    "    train_score = model.score(x_tr, y_tr) * 100\n",
    "    test_score = model.score(x_te, y_te) * 100\n",
    "    best_model[name] = [test_score]\n",
    "    \n",
    "# bagging\n",
    "for i in range(1, 6):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(max_depth=i).fit(x_tr, y_tr)\n",
    "    best_model[f'bagging, max_depth={i}'] = [model.score(x_te, y_te)]\n",
    "    \n",
    "# boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier().fit(x_tr, y_tr)\n",
    "best_model['boosting'] = [model.score(x_te, y_te)]\n",
    "\n",
    "# stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [('rf', RandomForestClassifier()),\n",
    "             ('gb', GradientBoostingClassifier())]\n",
    "\n",
    "model = StackingClassifier(estimators=estimators,\n",
    "                          final_estimator=LogisticRegression())\n",
    "\n",
    "best_model['stacking'] = [model.fit(x_tr, y_tr).score(x_te, y_te)]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(best_model).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "838cca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 좋은 성능을 가진 모델은 knn2, 98.67% 입니다\n"
     ]
    }
   ],
   "source": [
    "a = pd.DataFrame(best_model).T.sort_values(by = 0, ascending=False).reset_index().iloc[0,0]\n",
    "b = pd.DataFrame(best_model).T.sort_values(by = 0, ascending=False).reset_index().iloc[0,1]\n",
    "print(f'가장 좋은 성능을 가진 모델은 {a}, {b:.2f}% 입니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#조익준\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "best_model = {}\n",
    "\n",
    "# 데이터 분할\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(digits['data'],\n",
    "                                                    digits['target'],\n",
    "                                                    stratify=digits['target'],\n",
    "                                                    random_state=0)\n",
    "\n",
    "# 모델 설정\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=3)\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "dt3 = DecisionTreeClassifier(max_depth=3)\n",
    "dt5 = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "hard = VotingClassifier([('knn1', knn1), ('knn2', knn2), ('lr', lr), ('dt3', dt3), ('dt5', dt5)])\n",
    "\n",
    "soft = VotingClassifier([('knn1', knn1), ('knn2', knn2), ('lr', lr), ('dt3', dt3), ('dt5', dt5)], voting='soft')\n",
    "\n",
    "names = ['hard', 'soft', 'knn1', 'knn2', 'lr', 'dt3', 'dt5']\n",
    "for idx, model in enumerate([hard, soft, knn1, knn2, lr, dt3, dt5]):\n",
    "    model.fit(x_tr, y_tr)\n",
    "    name = names[idx]\n",
    "    train_score = model.score(x_tr, y_tr) * 100\n",
    "    test_score = model.score(x_te, y_te) * 100\n",
    "    best_model[name] = [test_score]\n",
    "    \n",
    "# bagging\n",
    "for i in range(1, 6):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(max_depth=i).fit(x_tr, y_tr)\n",
    "    best_model[f'bagging, max_depth={i}'] = [model.score(x_te, y_te)]\n",
    "    \n",
    "# boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier().fit(x_tr, y_tr)\n",
    "best_model['boosting'] = [model.score(x_te, y_te)]\n",
    "\n",
    "# stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [('rf', RandomForestClassifier()),\n",
    "             ('gb', GradientBoostingClassifier())]\n",
    "\n",
    "model = StackingClassifier(estimators=estimators,\n",
    "                          final_estimator=LogisticRegression())\n",
    "\n",
    "best_model['stacking'] = [model.fit(x_tr, y_tr).score(x_te, y_te)]\n",
    "\n",
    "import pandas as pd\n",
    "best_model_df = pd.DataFrame(best_model).T\n",
    "print(best_model_df)\n",
    "print(best_model_df.sort_values(0, ascending=False).reset_index())\n",
    "best_model_df.sort_values(0, ascending=False).reset_index().loc[0,\"index\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
